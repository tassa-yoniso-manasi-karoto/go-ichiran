package ichiran

import (
	"os"
	"strings"
	"testing"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)

const (
	// ICHIRAN_ADVANCED_TEST should be set to "1" to run advanced tests
	// These tests use longer and more complex Japanese text
	advancedTestEnvVar = "ICHIRAN_ADVANCED_TEST"
)

// skipIfNotAdvancedTest skips the test if ICHIRAN_ADVANCED_TEST is not set to "1"
func skipIfNotAdvancedTest(t *testing.T) {
	if os.Getenv(advancedTestEnvVar) != "1" {
		t.Skip("skipping advanced test; set ICHIRAN_ADVANCED_TEST=1 to run")
	}

	// Also skip if manual tests are not enabled (we need Docker)
	if os.Getenv("ICHIRAN_MANUAL_TEST") != "1" {
		t.Skip("skipping test that requires Docker; set ICHIRAN_MANUAL_TEST=1 to run")
	}
}

// TestComplexSentenceWithNestedClauses tests a complex sentence with nested clauses
func TestComplexSentenceWithNestedClauses(t *testing.T) {
	skipIfNotAdvancedTest(t)

	// Initialize Ichiran
	err := Init()
	require.NoError(t, err)

	// Complex sentence with nested clauses, quotes, and multiple types of punctuation
	// "Yesterday, I suddenly dropped by a bookstore, found a difficult philosophy book, and my heart trembled."
	japaneseText := "æ˜¨æ—¥ã€ãµã¨ç«‹ã¡å¯„ã£ãŸæœ¬å±‹ã§ã€é›£è§£ãªå“²å­¦æ›¸ã‚’è¦‹ã¤ã‘ã€å¿ƒãŒéœ‡ãˆã¾ã—ãŸã€‚"

	// Analyze the text
	tokensPtr, err := Analyze(japaneseText)
	require.NoError(t, err)

	// Verify we have a reasonable number of tokens
	tokens := *tokensPtr
	assert.GreaterOrEqual(t, len(tokens), 15, "Should have at least 15 tokens for complex sentence")

	// Count punctuation marks
	var commaCount, periodCount int
	for _, token := range tokens {
		if token.Surface == "ã€" || token.Surface == ", " {
			commaCount++
		} else if token.Surface == "ã€‚" || token.Surface == ". " {
			periodCount++
		}
	}

	// Should have 3 commas and 1 period
	assert.Equal(t, 3, commaCount, "Should have 3 commas in the sentence")
	assert.Equal(t, 1, periodCount, "Should have 1 period in the sentence")

	// Test various transformations
	t.Run("Complex Transformations", func(t *testing.T) {
		// Check selective transliteration with different thresholds
		lowResult, err := tokensPtr.SelectiveTranslit(50)
		require.NoError(t, err)

		mediumResult, err := tokensPtr.SelectiveTranslit(500)
		require.NoError(t, err)

		highResult, err := tokensPtr.SelectiveTranslit(2000)
		require.NoError(t, err)

		// Results should differ
		assert.NotEqual(t, lowResult, highResult, "Low and high threshold results should differ")
		assert.NotEqual(t, mediumResult, highResult, "Medium and high threshold results should differ")

		// Verify complete mapping
		mapping, err := tokensPtr.SelectiveTranslitFullMapping(1000)
		require.NoError(t, err)

		// Should have tokens for each significant part
		assert.NotEmpty(t, mapping.Text)
		assert.NotEmpty(t, mapping.Tokens)

		// Count various token statuses
		var preserved, transliterated, nonKanji int
		for _, token := range mapping.Tokens {
			switch token.Status {
			case StatusPreserved:
				preserved++
			case StatusIrregular, StatusInfrequent, StatusUnmappable:
				transliterated++
			case StatusNotKanji:
				nonKanji++
			}
		}

		// With this complex sentence, we should have tokens in each category
		assert.Greater(t, preserved, 0, "Should have preserved tokens")
		assert.Greater(t, nonKanji, 0, "Should have non-kanji tokens")

		// Romanization should contain spaces and punctuation
		roman := tokensPtr.Roman()
		assert.Contains(t, roman, " ") // Should have spaces
		assert.Contains(t, roman, ",") // Should represent commas
		assert.Contains(t, roman, ".") // Should represent periods
	})
}

// TestMixedLanguageText tests text that contains Japanese and non-Japanese elements
func TestMixedLanguageText(t *testing.T) {
	skipIfNotAdvancedTest(t)

	// Initialize Ichiran
	err := Init()
	require.NoError(t, err)

	// Mixed text with English, numbers, and Japanese
	// "I bought a new iPhone 13 in Tokyo last week for Â¥150,000."
	japaneseText := "å…ˆé€±ã€æ±äº¬ã§æ–°ã—ã„iPhone 13ã‚’Â¥150,000ã§è²·ã„ã¾ã—ãŸã€‚"

	// Analyze the text
	tokensPtr, err := Analyze(japaneseText)
	require.NoError(t, err)

	// Verify we get tokens for both Japanese and non-Japanese parts
	tokens := *tokensPtr

	// Check for proper handling of English words and numbers
	var foundEnglish, foundNumber bool

	// Debug log all tokens
	for i, token := range tokens {
		t.Logf("Token %d: '%s'", i, token.Surface)

		// Check for English words
		if strings.Contains(token.Surface, "iPhone") || strings.Contains(token.Surface, "13") {
			foundEnglish = true
		}

		// Check for currency and numbers (they might be combined in various ways)
		if strings.Contains(token.Surface, "Â¥") || strings.Contains(token.Surface, "150") ||
			strings.Contains(token.Surface, "150,000") {
			foundNumber = true
		}
	}

	assert.True(t, foundEnglish, "Should properly tokenize English words")
	assert.True(t, foundNumber, "Should properly tokenize numbers and currency symbols")

	// Romanization should contain some of the original elements
	roman := tokensPtr.Roman()
	t.Logf("Romanized text: %s", roman)

	// Check for presence of elements rather than exact strings
	assert.True(t, strings.Contains(roman, "iPhone") || strings.Contains(roman, "iphone") ||
		strings.Contains(roman, "phone"), "Should contain phone reference in romanization")
	assert.True(t, strings.Contains(roman, "150") || strings.Contains(roman, "yen") ||
		strings.Contains(roman, "Â¥"), "Should contain number or currency reference in romanization")
}

// TestLongArticleText tests analysis of a longer Japanese news article
func TestLongArticleText(t *testing.T) {
	skipIfNotAdvancedTest(t)

	// Initialize Ichiran
	err := Init()
	require.NoError(t, err)

	// Longer Japanese text (excerpt from a news article)
	japaneseText := `æ—¥æœ¬ã®ç§‘å­¦è€…ãŸã¡ã¯ã€åœ°çƒæ¸©æš–åŒ–ã®å½±éŸ¿ã§æµ·æ°´æ¸©ãŒä¸Šæ˜‡ã—ã¦ã„ã‚‹ã“ã¨ã«è­¦é˜ã‚’é³´ã‚‰ã—ã¦ã„ã¾ã™ã€‚
æœ€æ–°ã®ç ”ç©¶ã«ã‚ˆã‚‹ã¨ã€éå»50å¹´é–“ã§æ—¥æœ¬å‘¨è¾ºã®æµ·æ°´æ¸©ã¯ç´„1.2åº¦ä¸Šæ˜‡ã—ã¦ãŠã‚Šã€ã“ã‚Œã«ã‚ˆã£ã¦æ—¥æœ¬ã®æ°—å€™ã ã‘ã§ãªãã€
æµ·æ´‹ç”Ÿæ…‹ç³»ã«ã‚‚å¤§ããªå¤‰åŒ–ãŒèµ·ãã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã£ã¦ã„ã¾ã™ã€‚ç‰¹ã«ã€å—ã®æµ·åŸŸã§ã¯ã‚µãƒ³ã‚´ã®ç™½åŒ–ç¾è±¡ãŒé€²è¡Œã—ã€
åŒ—ã®æµ·åŸŸã§ã¯ä»¥å‰ã¯è¦‹ã‚‰ã‚Œãªã‹ã£ãŸç†±å¸¯é­šã®å­˜åœ¨ãŒç¢ºèªã•ã‚Œã¦ã„ã¾ã™ã€‚

ç ”ç©¶ãƒãƒ¼ãƒ ã®ãƒªãƒ¼ãƒ€ãƒ¼ã§ã‚ã‚‹ç”°ä¸­æ•™æˆã¯ã€Œã“ã®ã¾ã¾æ¸©æš–åŒ–ãŒé€²ã‚ã°ã€æ—¥æœ¬ã®æ¼æ¥­ã«ã‚‚æ·±åˆ»ãªå½±éŸ¿ãŒå‡ºã‚‹ã§ã—ã‚‡ã†ã€ã¨è­¦å‘Šã—ã¦ã„ã¾ã™ã€‚
å®Ÿéš›ã€ä¼çµ±çš„ãªæ¼å ´ã§ã¯é­šã®ç¨®é¡ã‚„é‡ã«å¤‰åŒ–ãŒè¦‹ã‚‰ã‚Œã€æ¼æ¥­ã‚’ç”Ÿæ¥­ã¨ã™ã‚‹åœ°åŸŸç¤¾ä¼šã«å½±éŸ¿ã‚’ä¸ãˆå§‹ã‚ã¦ã„ã¾ã™ã€‚

æ”¿åºœã¯å¯¾ç­–ã¨ã—ã¦ã€å†ç”Ÿå¯èƒ½ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®ä¿ƒé€²ã‚„ç‚­ç´ æ’å‡ºé‡ã®å‰Šæ¸›ç›®æ¨™ã‚’æ²ã’ã¦ã„ã¾ã™ãŒã€å°‚é–€å®¶ãŸã¡ã¯ã‚ˆã‚Šè¿…é€Ÿã‹ã¤å…·ä½“çš„ãªè¡Œå‹•ã‚’æ±‚ã‚ã¦ã„ã¾ã™ã€‚
ã€Œç§ãŸã¡ã«ã¯æ™‚é–“ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ä»Šã™ãã«è¡Œå‹•ã‚’èµ·ã“ã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€ã¨ç’°å¢ƒNGOã®ä»£è¡¨ã¯è¿°ã¹ã¦ã„ã¾ã™ã€‚`

	// Analyze the text - due to Ichiran API limitations, we need to process each paragraph separately
	var allTokens JSONTokens

	// Split by paragraphs and process each
	paragraphs := strings.Split(japaneseText, "\n")
	for _, para := range paragraphs {
		if strings.TrimSpace(para) == "" {
			continue
		}

		tokensPtr, err := Analyze(para)
		require.NoError(t, err)

		// Append tokens from this paragraph
		tokens := *tokensPtr
		allTokens = append(allTokens, tokens...)
	}

	// We don't need the pointer for the assertions, just the slice directly

	// Should have a large number of tokens for this long text
	tokens := allTokens
	assert.Greater(t, len(tokens), 50, "Should have many tokens for long article")

	// Since we're processing paragraphs separately, paragraph breaks are handled manually

	// Verify quotes are properly handled
	var quoteCount int
	for _, token := range tokens {
		// Log each token for debugging
		t.Logf("Token: '%s'", token.Surface)

		// Look for quote marks in various forms
		if strings.Contains(token.Surface, "ã€Œ") ||
			strings.Contains(token.Surface, "ã€") ||
			strings.Contains(token.Surface, "\"") {
			quoteCount++
		}
	}

	// We should find at least some quotes
	assert.Greater(t, quoteCount, 0, "Should find some quotation marks in the text")

	// Check handling of numbers and specialized terms
	var foundNumbers bool
	for _, token := range tokens {
		if strings.Contains(token.Surface, "1.2") ||
			strings.Contains(token.Surface, "50") ||
			strings.Contains(token.Surface, "1") ||
			strings.Contains(token.Surface, "2") {
			foundNumbers = true
			t.Logf("Found number token: '%s'", token.Surface)
			break
		}
	}
	assert.True(t, foundNumbers, "Should properly handle numeric values")
}

// TestSpecializedVocabulary tests analysis of text with technical/specialized vocabulary
func TestSpecializedVocabulary(t *testing.T) {
	skipIfNotAdvancedTest(t)

	// Initialize Ichiran
	err := Init()
	require.NoError(t, err)

	// Text with specialized medical and technical vocabulary
	japaneseText := "äººå·¥çŸ¥èƒ½ã«ã‚ˆã‚‹ç”»åƒè¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ ã‚’ç”¨ã„ã¦ã€æ—©æœŸæ®µéšã§ã®æ‚ªæ€§è…«ç˜ã®æ¤œå‡ºç‡ãŒå‘ä¸Šã—ã¾ã—ãŸã€‚é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®ç ”ç©¶é€²å±•ã«ã‚ˆã£ã¦ã€å°†æ¥çš„ã«ã¯å‰µè–¬ãƒ—ãƒ­ã‚»ã‚¹ã‚‚å¤§å¹…ã«åŠ¹ç‡åŒ–ã•ã‚Œã‚‹ã§ã—ã‚‡ã†ã€‚"

	// Analyze the text
	tokensPtr, err := Analyze(japaneseText)
	require.NoError(t, err)

	// Check for technical terms
	tokens := *tokensPtr

	// Define technical terms to look for
	technicalTerms := []string{
		"äººå·¥çŸ¥èƒ½", // artificial intelligence
		"ç”»åƒè¨ºæ–­", // image diagnosis
		"æ‚ªæ€§è…«ç˜", // malignant tumor
		"é‡å­",   // quantum
	}

	// Count how many technical terms we found
	var foundTermsCount int
	for _, term := range technicalTerms {
		for _, token := range tokens {
			if token.Surface == term {
				foundTermsCount++
				break
			}
		}
	}

	// Should find at least some of our technical terms
	assert.GreaterOrEqual(t, foundTermsCount, 2,
		"Should identify specialized technical vocabulary")

	// Gloss information should be present for technical terms
	var glossFound bool
	for _, token := range tokens {
		if len(token.Gloss) > 0 {
			glossFound = true
			break
		}
	}
	assert.True(t, glossFound, "Should have gloss information for technical terms")
}

// TestClassicalJapaneseText tests analysis of classical/literary Japanese
func TestClassicalJapaneseText(t *testing.T) {
	skipIfNotAdvancedTest(t)

	// Initialize Ichiran
	err := Init()
	require.NoError(t, err)

	// Classical Japanese text (from the Tale of Genji, opening lines)
	japaneseText := "ã„ã¥ã‚Œã®å¾¡æ™‚ã«ã‹ã€å¥³å¾¡ã€æ›´è¡£ã‚ã¾ãŸã•ã¶ã‚‰ã²ãŸã¾ã²ã‘ã‚‹ä¸­ã«ã€ã„ã¨ã‚„ã‚€ã”ã¨ãªãéš›ã«ã¯ã‚ã‚‰ã¬ãŒã€ã™ãã‚Œã¦æ™‚ã‚ããŸã¾ãµã‚ã‚Šã‘ã‚Šã€‚"

	// Analyze the text
	tokensPtr, err := Analyze(japaneseText)
	require.NoError(t, err)

	// Even with classical Japanese, we should get tokenization
	tokens := *tokensPtr
	assert.NotEmpty(t, tokens, "Should tokenize classical Japanese")

	// Get token count
	tokenCount := len(tokens)

	// Log the number of tokens to help with debugging
	t.Logf("Found %d tokens in classical Japanese text", tokenCount)

	// Classical Japanese should still produce readings
	readings := tokensPtr.Kana()
	assert.NotEmpty(t, readings, "Should produce readings even for classical Japanese")

	// Get romanized form
	roman := tokensPtr.Roman()
	assert.NotEmpty(t, roman, "Should produce romanization even for classical Japanese")
}

// TestEdgeCases tests various edge cases and unusual inputs
func TestEdgeCases(t *testing.T) {
	skipIfNotAdvancedTest(t)

	// Initialize Ichiran
	err := Init()
	require.NoError(t, err)

	// Test cases with edge case inputs
	testCases := []struct {
		name     string
		input    string
		expected int // minimum expected token count
	}{
		{
			name:     "very short input",
			input:    "çŒ«ã€‚",
			expected: 2, // "çŒ«" and "ã€‚"
		},
		{
			name:     "repeating characters",
			input:    "ã‚ãã‚ããƒ‰ã‚­ãƒ‰ã‚­ï¼",
			expected: 3, // "ã‚ãã‚ã", "ãƒ‰ã‚­ãƒ‰ã‚­", "ï¼"
		},
		{
			name:     "unusual punctuation",
			input:    "ã€Œãˆã£ï¼Ÿã€ã€Œã¯ãâ€¦ã€ï¼ˆè€ƒãˆä¸­ï¼‰",
			expected: 9, // Each punctuation mark and word should be tokenized
		},
		{
			name:     "emoji and symbols",
			input:    "ä»Šæ—¥ã‚‚é ‘å¼µã‚Šã¾ã—ã‚‡ã†ï¼ğŸ‘âœ¨",
			expected: 4, // Sentence + emojis (may vary based on how ichiran handles emojis)
		},
		{
			name:     "rare kanji",
			input:    "ğ ®·é‡å®¶ã§é£Ÿäº‹ã‚’ã—ãŸã€‚", // Uses uncommon/variant kanji for "Yoshinoya"
			expected: 5,
		},
		{
			name:     "repeated punctuation",
			input:    "ãˆãˆãˆï¼ï¼Ÿï¼ï¼Ÿ",
			expected: 2,
		},
	}

	for _, tc := range testCases {
		t.Run(tc.name, func(t *testing.T) {
			// Analyze the text
			tokensPtr, err := Analyze(tc.input)
			require.NoError(t, err, "Analysis should not fail")

			// Verify token count
			tokens := *tokensPtr
			assert.GreaterOrEqual(t, len(tokens), tc.expected,
				"Should have expected minimum token count")

			// Check basic transformations
			romanized := tokensPtr.Roman()
			assert.NotEmpty(t, romanized, "Should produce some romanization")

			kana := tokensPtr.Kana()
			assert.NotEmpty(t, kana, "Should produce kana readings")
		})
	}
}

// TestComplexGrammaticalStructures tests parsing of sentences with complex grammar
func TestComplexGrammaticalStructures(t *testing.T) {
	skipIfNotAdvancedTest(t)

	// Initialize Ichiran
	err := Init()
	require.NoError(t, err)

	// Text with complex grammatical structures, conditional clauses, passive voice, etc.
	japaneseText := "ã‚‚ã—ç§ãŒèª˜ã‚ã‚Œã¦ã„ãªã‹ã£ãŸã‚‰ã€ãã®ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ¼ã«è¡Œã‹ãªã‹ã£ãŸã§ã—ã‚‡ã†ã—ã€ã‚ãªãŸã«ã‚‚ä¼šãˆãªã‹ã£ãŸã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ç‰©äº‹ã¯æ™‚ã€…ã€äºˆæƒ³ã‚‚ã—ãªã‹ã£ãŸå½¢ã§å±•é–‹ã™ã‚‹ã‚‚ã®ã§ã™ã­ã€‚"

	// Analyze the text
	tokensPtr, err := Analyze(japaneseText)
	require.NoError(t, err)

	// Verify we get tokens
	tokens := *tokensPtr
	assert.NotEmpty(t, tokens, "Should tokenize text with complex grammar")

	// Look for grammatical elements
	var foundConditional, foundPassive, foundNegative, foundPotential bool

	for _, token := range tokens {
		// Check surface form for indicators
		if token.Surface == "ãŸã‚‰" || token.Surface == "ã°" || token.Surface == "ãªã‚‰" {
			foundConditional = true
		}

		if strings.Contains(token.Surface, "ã‚Œ") && (strings.Contains(token.Surface, "ã„ã‚‹") || strings.Contains(token.Surface, "ã„ãŸ")) {
			foundPassive = true
		}

		if strings.Contains(token.Surface, "ãªã‹ã£") || strings.Contains(token.Surface, "ãªã„") || strings.Contains(token.Surface, "ãš") {
			foundNegative = true
		}

		if strings.Contains(token.Surface, "ãˆã‚‹") || strings.Contains(token.Surface, "ã‚Œã‚‹") {
			foundPotential = true
		}

		// Also check conjugation data
		for _, conj := range token.Conj {
			for _, prop := range conj.Prop {
				if prop.Type == "conditional" {
					foundConditional = true
				}
				if prop.Type == "passive" {
					foundPassive = true
				}
				if prop.Neg {
					foundNegative = true
				}
				if prop.Type == "potential" {
					foundPotential = true
				}
			}
		}
	}

	// We should find at least some of these grammatical structures
	// (Note: exact identification depends on ichiran's capabilities)
	assert.True(t, foundConditional || foundPassive || foundNegative || foundPotential,
		"Should identify some complex grammatical structures")
}
